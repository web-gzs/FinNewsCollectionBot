
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os


# =========================
# ç¯å¢ƒå˜é‡é…ç½®
# =========================

# ä¼˜å…ˆç”¨ DeepSeek keyï¼ˆæ¨èï¼‰ï¼Œæ²¡æœ‰åˆ™å›é€€åˆ° OPENAI_API_KEY
deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")
api_key = deepseek_api_key or openai_api_key

if not api_key:
    raise ValueError("æœªè®¾ç½® API Keyï¼šè¯·åœ¨ Github Actions ä¸­è®¾ç½® DEEPSEEK_API_KEYï¼ˆæ¨èï¼‰æˆ– OPENAI_API_KEYã€‚")

# DeepSeek OpenAI-compatible base_urlï¼ˆå¯è¦†ç›–ï¼‰
BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com/v1")

# æ¨¡å‹åï¼ˆå¯è¦†ç›–ï¼‰
MODEL_NAME = os.getenv("LLM_MODEL", "deepseek-chat")

# ä»ç¯å¢ƒå˜é‡è·å– Serveré…± SendKeys
server_chan_keys_env = os.getenv("SERVER_CHAN_KEYS")
if not server_chan_keys_env:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")
server_chan_keys = [k.strip() for k in server_chan_keys_env.split(",") if k.strip()]

openai_client = OpenAI(api_key=api_key, base_url=BASE_URL)


# =========================
# RSSæºåœ°å€åˆ—è¡¨
# =========================
rss_feeds = {
    "ğŸ’² åå°”è¡—è§é—»": {
        "åå°”è¡—è§é—»": "https://dedicated.wallstreetcn.com/rss.xml",
    },
    "ğŸ’» 36æ°ª": {
        "36æ°ª": "https://36kr.com/feed",
    },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±": "https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ": "http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹": "http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘": "https://www.chinanews.com.cn/rss/finance.xml",
        "å›½å®¶ç»Ÿè®¡å±€-æœ€æ–°å‘å¸ƒ": "https://www.stats.gov.cn/sj/zxfb/rss.xml",
    },
    "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/WSJcomUSBusiness",
        "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº": "https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
        "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
        "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
        "ETF Trends": "https://www.etftrends.com/feed/",
    },
    "ğŸŒ ä¸–ç•Œç»æµ": {
        "åå°”è¡—æ—¥æŠ¥ - ç»æµ": "https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
        "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
    },
}


# =========================
# å·¥å…·å‡½æ•°
# =========================

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()


# çˆ¬å–ç½‘é¡µæ­£æ–‡ (ç”¨äº AI åˆ†æï¼Œä½†ä¸å±•ç¤º)
def fetch_article_text(url, max_len=1500):
    try:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = (article.text or "").strip()
        if not text:
            print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
            return ""
        return text[:max_len]  # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡º API è¾“å…¥é™åˆ¶
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return ""


# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/91.0.4472.124 Safari/537.36"
        )
    }
    return feedparser.parse(url, request_headers=headers)


# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, "entries") and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
        time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None


# è·å–RSSå†…å®¹ï¼ˆçˆ¬å–æ­£æ–‡ä½†ä¸å±•ç¤ºï¼‰
def fetch_rss_articles(rss_feeds, max_articles=5):
    news_data = {}
    analysis_text = ""  # ç”¨äºAIåˆ†æçš„æ­£æ–‡å†…å®¹

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, url in sources.items():
            print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")
            feed = fetch_feed_with_retry(url)
            if not feed:
                print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                continue
            print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

            articles = []
            for entry in feed.entries[:max_articles]:
                title = entry.get("title", "æ— æ ‡é¢˜")
                link = entry.get("link", "") or entry.get("guid", "")
                if not link:
                    print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                    continue

                # çˆ¬å–æ­£æ–‡ç”¨äºåˆ†æï¼ˆä¸å±•ç¤ºï¼‰
                article_text = fetch_article_text(link)
                if article_text:
                    analysis_text += f"\n{article_text}\n\n"

                print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                articles.append(f"- [{title}]({link})")

                # è½»å¾®é™é€Ÿï¼Œå‡å°‘è¢«åçˆ¬æ¦‚ç‡
                time.sleep(0.3)

            if articles:
                category_content += f"### {source}\n" + "\n".join(articles) + "\n\n"

        news_data[category] = category_content

    return news_data, analysis_text


# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    # å¦‚æœæ²¡æŠ“åˆ°æ­£æ–‡ï¼Œå°±ä¸è°ƒæ¨¡å‹ï¼ˆçœé’±&é¿å…æŠ¥é”™ï¼‰
    if not text.strip():
        return "ï¼ˆæœªæŠ“å–åˆ°å¯ç”¨äºåˆ†æçš„æ­£æ–‡å†…å®¹ï¼Œæœ¬æ¬¡ä»…æ¨é€æ ‡é¢˜ä¸é“¾æ¥ã€‚ï¼‰"

    try:
        completion = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {
                    "role": "system",
                    "content": """
ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š
1. æå–æ–°é—»ä¸­æ¶‰åŠçš„ä¸»è¦è¡Œä¸šå’Œä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šæˆ–ä¸»é¢˜ï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸š/ä¸»é¢˜ã€‚ï¼ˆå¦‚æ–°é—»æœªæä¾›å…·ä½“æ¶¨å¹…ï¼Œè¯·ç»“åˆæè¿°å’Œå¸‚åœºæƒ…ç»ªæ¨æµ‹çƒ­ç‚¹ï¼‰
2. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹ï¼Œè¾“å‡ºï¼š
   - å‚¬åŒ–å‰‚ï¼šåˆ†æè¿‘æœŸä¸Šæ¶¨çš„å¯èƒ½åŸå› ï¼ˆæ”¿ç­–ã€æ•°æ®ã€äº‹ä»¶ã€æƒ…ç»ªç­‰ï¼‰ã€‚
   - å¤ç›˜ï¼šæ¢³ç†è¿‡å»3ä¸ªæœˆè¯¥è¡Œä¸š/ä¸»é¢˜çš„æ ¸å¿ƒé€»è¾‘ã€å…³é”®åŠ¨æ€ä¸é˜¶æ®µæ€§èµ°åŠ¿ã€‚
   - å±•æœ›ï¼šåˆ¤æ–­è¯¥çƒ­ç‚¹æ˜¯çŸ­æœŸç‚’ä½œè¿˜æ˜¯æœ‰æŒç»­è¡Œæƒ…æ½œåŠ›ã€‚
3. å°†ä»¥ä¸Šåˆ†ææ•´åˆä¸ºä¸€ç¯‡1500å­—ä»¥å†…çš„è´¢ç»çƒ­ç‚¹æ‘˜è¦ï¼Œé€»è¾‘æ¸…æ™°ã€é‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…é˜…è¯»ã€‚
""".strip(),
                },
                {"role": "user", "content": text},
            ],
        )
        return completion.choices[0].message.content.strip()

    except Exception as e:
        # å…³é”®ï¼šLLM è°ƒç”¨å¤±è´¥ä¹Ÿä¸è¦è®©æ•´ä¸ª workflow å¤±è´¥
        # æ¯”å¦‚ï¼š402 Insufficient Balance / 429 / ç½‘ç»œé”™è¯¯ç­‰
        print(f"âŒ AI æ€»ç»“å¤±è´¥ï¼š{repr(e)}")
        return (
            "ï¼ˆAI æ€»ç»“å¤±è´¥ï¼šå¯èƒ½æ˜¯ä½™é¢ä¸è¶³/é™æµ/ç½‘ç»œé—®é¢˜ã€‚æœ¬æ¬¡ä»…æ¨é€æ ‡é¢˜ä¸é“¾æ¥ã€‚"
            "ä½ å¯ä»¥æ£€æŸ¥ API è´¦æˆ·ä½™é¢æˆ–é™ä½æŠ“å–æ•°é‡åé‡è¯•ã€‚ï¼‰"
        )


# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key in server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        try:
            response = requests.post(url, data=data, timeout=10)
            if response.ok:
                print(f"âœ… æ¨é€æˆåŠŸ: {key}")
            else:
                print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")
        except Exception as e:
            print(f"âŒ æ¨é€å¼‚å¸¸: {key}, é”™è¯¯ï¼š{e}")


# =========================
# ä¸»ç¨‹åº
# =========================
if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š N ç¯‡æ–‡ç« ï¼ˆå¯è°ƒæ•´ï¼‰
    MAX_PER_SOURCE = int(os.getenv("MAX_PER_SOURCE", "5"))

    articles_data, analysis_text = fetch_rss_articles(rss_feeds, max_articles=MAX_PER_SOURCE)

    # AIç”Ÿæˆæ‘˜è¦ï¼ˆå¤±è´¥ä¼šé™çº§ï¼Œä¸ä¼šè®© workflow å¤±è´¥ï¼‰
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # æ¨é€åˆ°å¤šä¸ªserveré…±key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
